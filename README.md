# deep-learning-challenge
During this project, I used my knowledge of machine learning and neural networks to use the features in the provided dataset to create a binary classifier that can predict whether applicants will be successful if funded by Alphabet Soup. First, for Step 1, I preprocessed the data. Using my knowledge of Pandas and scikit-learn's StandardScaler(), I preprocessed the dataset. First, I read in the charity_data.csv to a Pandas DataFrame. Next, I dropped the EIN and NAME columns. Then, I determined the number of unique values for each column. I then used the number of data points for each unique value to pick a cutoff point to bin "rare" categorical variables together in a new value, Other, and then checked if the binning was successful. Next, I used pd.get_dummies() to encode categorical variables. Then, I split the preprocessed data into a features array, X, and a target array, y. I used these arrays and the train_test_split function to split the data into training and testing datasets. I scaled the training and testing features datasets by creating a StandardScaler instance, fitting it to the training data, then using the transform function. For Step 2, I compiled, trained and evaluated the model. Using my knowledge of TensorFlow, I designed a neural network, or deep learning model, to create a binary classification model that can predict if an Alphabet Soup-funded organization will be successful based on the features in the dataset. First, I continued using the Jupyter Notebook in which I performed the preprocessing steps from Step 1. Then, I created a neural network model by assigning the number of input features and nodes for each layer using TensorFlow and Keras. Then, I created the first hidden layer and chose an appropriate activation function. Then, I created an output layer with an appropriate activation function and checked the structure of the model. After compiling and training the model, I created a callback that saves the model's weights every five epochs. I evaluated the model using the test data to determine the loss and accuracy. Next, I saved and exported the results to an HDF5 file and named the file AlphabetSoupCharity.h5. For Step 3, I optimized the model. Using my knowledge of TensorFlow, I optimized my model to achieve a target predictive accuracy higher than 75%. For Step 4, I wrote a report on the neural network model, Neural_Network_Model_report.pdf.

# Resources
This folder contains charity_data.csv, a CSV containing more than 34,000 organizations that have received funding from Alphabet Soup over the years.

# Models
This folder contains AlphabetSoupCharity1.h5, AlphabetSoupCharity2.h5 and AlphabetSoupCharity3.h5.
